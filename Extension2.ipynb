{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T14:52:17.689577Z",
     "iopub.status.busy": "2025-02-15T14:52:17.689259Z",
     "iopub.status.idle": "2025-02-15T14:52:35.616020Z",
     "shell.execute_reply": "2025-02-15T14:52:35.615067Z",
     "shell.execute_reply.started": "2025-02-15T14:52:17.689549Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install numpy pandas tqdm  scikit-learn nltk datasets transformers torch keybert keyphrase-vectorizers sentence_transformers wikipedia gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T14:52:35.617546Z",
     "iopub.status.busy": "2025-02-15T14:52:35.617284Z",
     "iopub.status.idle": "2025-02-15T14:53:09.487248Z",
     "shell.execute_reply": "2025-02-15T14:53:09.486032Z",
     "shell.execute_reply.started": "2025-02-15T14:52:35.617514Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import string\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import random\n",
    "import gc\n",
    "from collections import Counter, OrderedDict\n",
    "import re\n",
    "from pprint import pprint\n",
    "import configparser\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import product, chain\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "import torch\n",
    "import gradio as gr\n",
    "import wikipedia as wiki\n",
    "\n",
    "# Hugging Face imports\n",
    "from huggingface_hub import login\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "\n",
    "# KeyBERT and Sentence Transformers imports\n",
    "from keybert import KeyBERT, KeyLLM\n",
    "from keybert.llm import TextGeneration\n",
    "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-02-15T14:53:09.490053Z",
     "iopub.status.busy": "2025-02-15T14:53:09.489655Z",
     "iopub.status.idle": "2025-02-15T14:53:27.029534Z",
     "shell.execute_reply": "2025-02-15T14:53:27.028389Z",
     "shell.execute_reply.started": "2025-02-15T14:53:09.490011Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T14:53:27.031428Z",
     "iopub.status.busy": "2025-02-15T14:53:27.031039Z",
     "iopub.status.idle": "2025-02-15T14:53:27.308635Z",
     "shell.execute_reply": "2025-02-15T14:53:27.308012Z",
     "shell.execute_reply.started": "2025-02-15T14:53:27.031390Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('./config.ini')\n",
    "HF_TOKEN = config['hf_token']['access_token']  # For Hugging Face\n",
    "login(HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test QA model\n",
    "\n",
    "* google-bert/bert-large-cased-whole-word-masking-finetuned-squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T11:46:52.061551Z",
     "iopub.status.busy": "2025-02-15T11:46:52.061257Z",
     "iopub.status.idle": "2025-02-15T11:46:52.070231Z",
     "shell.execute_reply": "2025-02-15T11:46:52.069658Z",
     "shell.execute_reply.started": "2025-02-15T11:46:52.061524Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def normalize_text(s):\n",
    "    \"\"\"\n",
    "    Normalizes a string by:\n",
    "    - Converting to lowercase\n",
    "    - Removing punctuation\n",
    "    - Removing articles ('a', 'an', 'the')\n",
    "    - Removing extra spaces\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return \"\"\n",
    "\n",
    "    # Convert to lowercase\n",
    "    s = s.lower()\n",
    "    # Remove punctuation\n",
    "    s = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", s)\n",
    "    # Remove articles\n",
    "    s = re.sub(r\"\\b(a|an|the)\\b\", \" \", s)\n",
    "    # Remove extra spaces\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "def get_tokens_from_text(s):\n",
    "    \"\"\"Returns the normalized tokens from a string.\"\"\"\n",
    "    return normalize_text(s).split() if s else []\n",
    "\n",
    "def calculate_exact_match(predicted, true_answers):\n",
    "    \"\"\"\n",
    "    Calculates the Exact Match between the predicted answer and true answers.\n",
    "    Returns 1.0 if there is a match, otherwise 0.0.\n",
    "    \"\"\"\n",
    "    true_texts = true_answers['text'] if isinstance(true_answers, dict) else true_answers\n",
    "    for t in true_texts:\n",
    "        if normalize_text(predicted) == normalize_text(t):\n",
    "            return 1.0\n",
    "    return 0.0\n",
    "\n",
    "def calculate_f1_score(predicted, true_answers):\n",
    "    true_texts = true_answers['text'] if isinstance(true_answers, dict) else true_answers\n",
    "    f1_scores = []\n",
    "    for true_answer in true_texts:\n",
    "        pred_tokens = get_tokens_from_text(predicted)\n",
    "        true_tokens = get_tokens_from_text(true_answer)\n",
    "        common = Counter(pred_tokens) & Counter(true_tokens)\n",
    "        num_same = sum(common.values())\n",
    "        if num_same == 0:\n",
    "            f1_scores.append(0.0)\n",
    "            continue\n",
    "        precision = num_same / len(pred_tokens)\n",
    "        recall = num_same / len(true_tokens)\n",
    "        f1_scores.append((2 * precision * recall) / (precision + recall) if (precision + recall) else 0.0)\n",
    "    return max(f1_scores) if f1_scores else 0.0\n",
    "\n",
    "def calculate_metrics(results):\n",
    "    \"\"\"\n",
    "    Calculates the main metrics: Exact Match (EM), F1 score, and average confidence.\n",
    "    \"\"\"\n",
    "    exact_matches = []\n",
    "    f1_scores = []\n",
    "    confidences = []\n",
    "\n",
    "    for result in results:\n",
    "        pred_answer = result[\"predicted_answer\"]\n",
    "        true_answers = result[\"true_answers\"]\n",
    "        exact_matches.append(calculate_exact_match(pred_answer, true_answers))\n",
    "        f1_scores.append(calculate_f1_score(pred_answer, true_answers))\n",
    "        confidences.append(result[\"confidence\"])\n",
    "    \n",
    "    total = len(results)\n",
    "    eval_dict = OrderedDict([\n",
    "        ('exact', 100.0 * np.mean(exact_matches)),\n",
    "        ('f1', 100.0 * np.mean(f1_scores)),\n",
    "        ('avg_confidence', np.mean(confidences)),\n",
    "        ('total', total)\n",
    "    ])\n",
    "    \n",
    "    return eval_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the SQuAD dataset\n",
    "squad_dataset = load_dataset(\"rajpurkar/squad\") \n",
    "validation_set = squad_dataset['validation']\n",
    "validation_df = validation_set.to_pandas()[['context', 'question', 'answers']] \n",
    "\n",
    "# Create the question-answering pipeline for the selected model\n",
    "model = 'google-bert/bert-large-cased-whole-word-masking-finetuned-squad' \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, device=device) \n",
    "\n",
    "results = []  # List to store the results\n",
    "start_time = time.time()  # Record the start time for execution time measurement\n",
    "\n",
    "# Iterate over the records in the validation dataset and predict answers for each question\n",
    "for _, row in tqdm(validation_set.iterrows(), total=validation_set.shape[0], desc=f\"Model: {model}\"):\n",
    "    # Use the context and question to predict the answer using the QA pipeline\n",
    "    output = qa_pipeline(question=row['question'], context=row['context'])\n",
    "    \n",
    "    # Append the result for each prediction\n",
    "    results.append({\n",
    "        \"predicted_answer\": output['answer'], \n",
    "        \"true_answers\": row['answers'],\n",
    "        \"confidence\": output.get('score', 0.0) \n",
    "    })\n",
    "\n",
    "elapsed_time = time.time() - start_time \n",
    "\n",
    "# Compute the evaluation metrics for the model\n",
    "metrics = calculate_metrics(results)  # Function to calculate metrics like Exact Match and F1 Score\n",
    "metrics['execution_time'] = elapsed_time  # Add the execution time to the metrics\n",
    "\n",
    "# Print the results\n",
    "print(f\"Total records evaluated: {metrics['total']}\") \n",
    "print(f\"Exact Match: {metrics['exact']:.2f}%\")\n",
    "print(f\"F1 Score: {metrics['f1']:.2f}%\")\n",
    "print(f\"Average Confidence: {metrics['avg_confidence']:.3f}\")\n",
    "print(f\"Execution Time: {metrics['execution_time']:.2f} seconds\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# google-bert/bert-large-cased-whole-word-masking-finetuned-squad\n",
    "# Exact Match: 87.26%, F1 Score: 93.24%, Average Confidence: 0.703, Execution Time: 543.88 seconds\n",
    "# 0,0514 seconds per question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T15:12:17.588466Z",
     "iopub.status.busy": "2025-02-15T15:12:17.588108Z",
     "iopub.status.idle": "2025-02-15T15:12:17.609493Z",
     "shell.execute_reply": "2025-02-15T15:12:17.608675Z",
     "shell.execute_reply.started": "2025-02-15T15:12:17.588436Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "KEY_LLM_PROMPT = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "\n",
    "You are a helpful assistant specialized in extracting comma-separated keywords.\n",
    "You are to the point and only give the answer in isolation without any chat-based fluff.\n",
    "\n",
    "<</SYS>>\n",
    "I have the following document:\n",
    "- The website mentions that it only takes a couple of days to deliver but I still have not received mine.\n",
    "\n",
    "Please give me the keywords that are present in this document and separate them with commas.\n",
    "Make sure you to only return the keywords and say nothing else. For example, don't say:\n",
    "\"Here are the keywords present in the document\"\n",
    "[/INST] meat, beef, eat, eating, emissions, steak, food, health, processed, chicken [INST]\n",
    "\n",
    "I have the following document:\n",
    "- [DOCUMENT]\n",
    "\n",
    "With the following candidate keywords:\n",
    "- [CANDIDATES]\n",
    "\n",
    "Please give me the keywords that are present in this document and separate them with commas.\n",
    "Make sure you to only return the keywords and say nothing else. For example, don't say:\n",
    "\"Here are the keywords present in the document\"\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "def initialize_models(embedding_model, llm_model, qa_model, sum_model):\n",
    "    \"\"\"\n",
    "    Loads and initializes machine learning models for embeddings, keyword extraction, text generation, \n",
    "    question answering, and summarization, using GPU if available.\n",
    "\n",
    "    Args:\n",
    "        embedding_model (str): Model for sentence embeddings. Defaults to 'all-MiniLM-L6-v2'.\n",
    "        llm_model (str): Model for text generation. Defaults to 'gpt-2'.\n",
    "        qa_model (str): Model for question answering. Defaults to 'distilbert-base-cased-distilled-squad'.\n",
    "        sum_model (str): Model for summarization. Defaults to 'facebook/bart-large-cnn'.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Initialized models:\n",
    "            - SentenceTransformer for embeddings.\n",
    "            - KeyBERT for keyword extraction.\n",
    "            - KeyLLM for LLM-based keyword extraction.\n",
    "            - HuggingFace pipelines for question answering and summarization.\n",
    "    \"\"\"\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    try:\n",
    "        # Initialize the sentence transformer model\n",
    "        print(\"Loading Sentence Transformer model...\")\n",
    "        model = SentenceTransformer(embedding_model, device=device)\n",
    "        \n",
    "        # Initialize the KeyBERT model\n",
    "        print(\"Loading KeyBERT model...\")\n",
    "        kw_bert_model = KeyBERT(model)\n",
    "        \n",
    "        # Initialize the KeyLLM model\n",
    "        print(\"Loading KeyLLM model...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(llm_model)\n",
    "        llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "            llm_model,\n",
    "            trust_remote_code=True,\n",
    "            device_map='auto'\n",
    "        )\n",
    "        generator = pipeline(\n",
    "            model=llm_model, tokenizer=tokenizer,\n",
    "            task='text-generation',\n",
    "            max_new_tokens=50,\n",
    "            repetition_penalty=1.1,\n",
    "            model_kwargs={\"load_in_4bit\": True}\n",
    "        )\n",
    "        llm = TextGeneration(generator, prompt=KEY_LLM_PROMPT)  # KEYLLM_PROMPT global variable\n",
    "        kw_llm_model = KeyLLM(llm)\n",
    "            \n",
    "        # Initialize the question answering model\n",
    "        print(\"Loading Question Answering model...\")\n",
    "        question_answerer = pipeline(\"question-answering\", model=qa_model, device=device)\n",
    "        \n",
    "        # Initialize the summarization model\n",
    "        print(\"Loading Summarization model...\")\n",
    "        summarizer = pipeline(\"summarization\", model=sum_model, device=device)\n",
    "        \n",
    "        print(\"Models loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the models: {e}\")\n",
    "    \n",
    "    return model, kw_bert_model, kw_llm_model, question_answerer, summarizer\n",
    "\n",
    "def clean_document(document):\n",
    "    \"\"\"\n",
    "    Cleans the text of a document by removing unwanted characters and extra spaces.\n",
    "\n",
    "    Args:\n",
    "        document (str): The text of the document.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned text.\n",
    "    \"\"\"\n",
    "    # Ensure the input is a string, raise an error if it's not\n",
    "    if not isinstance(document, str):\n",
    "        raise ValueError(\"The document must be a string.\")\n",
    "\n",
    "    return re.sub(r\"\\s+\", \" \", document).strip()\n",
    "\n",
    "    \n",
    "def split_document(document, max_tokens, overlap_percentage):\n",
    "    \"\"\"\n",
    "    Splits a document into overlapping chunks of a specified maximum token length.\n",
    "\n",
    "    Args:\n",
    "        document (str): The input text to be split.\n",
    "        max_tokens (int): The maximum number of tokens allowed in each chunk.\n",
    "        overlap_percentage (float): The percentage of overlap between consecutive chunks (e.g., 0.1 for 10% overlap).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of text chunks, each containing up to max_tokens tokens, with the specified overlap.\n",
    "    \"\"\"\n",
    "    if not isinstance(document, str):\n",
    "        raise ValueError(\"Input document must be a string.\")\n",
    "    if max_tokens <= 0:\n",
    "        raise ValueError(\"max_tokens must be greater than 0.\")\n",
    "    if not (0 <= overlap_percentage < 1):\n",
    "        raise ValueError(\"overlap_percentage must be between 0 and 1.\")\n",
    "\n",
    "    # Tokenize and clean the document\n",
    "    tokens = word_tokenize(clean_document(document))\n",
    "    overlap = int(max_tokens * overlap_percentage)\n",
    "\n",
    "    # Ensure at least one token overlaps when overlap_percentage > 0\n",
    "    step = max(max_tokens - overlap, 1)\n",
    "    \n",
    "    # Generate chunks with overlap\n",
    "    chunks = [' '.join(tokens[i:i + max_tokens]) for i in range(0, len(tokens), step)]\n",
    "    return chunks\n",
    "\n",
    "def get_top_kw (doc, candidates, model, top_n):\n",
    "    \"\"\"\n",
    "    Get top keywords based on similarity to the document.\n",
    "    \n",
    "    Args:\n",
    "        doc: Input document text\n",
    "        candidates: List of candidate keywords\n",
    "        model: SentenceTransformer model for encoding\n",
    "        top_n: Number of top keywords to return\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples containing (keyword, similarity_score)\n",
    "    \"\"\"\n",
    "    # Encode document and candidates\n",
    "    doc_embedding = model.encode(doc).reshape(1, -1)\n",
    "    candidate_embeddings = model.encode(candidates)\n",
    "    \n",
    "    # Calculate cosine similarities using sklearn\n",
    "    similarities = cosine_similarity(candidate_embeddings, doc_embedding).flatten()\n",
    "    \n",
    "    # Get top keywords with their scores\n",
    "    top_indices = similarities.argsort()[-top_n:][::-1]\n",
    "    return [(candidates[idx], float(similarities[idx])) for idx in top_indices]\n",
    "    \n",
    "def extract_keywords_from_text(doc, kw_bert_model, kw_llm_model, model, use_keyllm, diversity, top_n):\n",
    "    \"\"\"\n",
    "    Extract keywords from text using KeyBERT and optionally KeyLLM.\n",
    "    \n",
    "    Args:\n",
    "        doc: Input text\n",
    "        kw_bert_model: KeyBERT model instance\n",
    "        kw_llm_model: KeyLLM model instance (optional)\n",
    "        model: SentenceTransformer model for encoding\n",
    "        use_keyllm: Whether to use KeyLLM for refinement\n",
    "        diversity: Diversity parameter for MMR\n",
    "        top_n: Number of keywords to extract\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples containing (keyword, score)\n",
    "    \"\"\"\n",
    "    def extract_with_keybert(top_n):\n",
    "        \"\"\"Helper function for KeyBERT extraction with error handling\"\"\"\n",
    "        try:\n",
    "            return kw_bert_model.extract_keywords(\n",
    "                docs=doc,\n",
    "                vectorizer=KeyphraseCountVectorizer(),\n",
    "                use_mmr=True,\n",
    "                diversity=diversity,\n",
    "                top_n=top_n\n",
    "            )\n",
    "        except ValueError as e:\n",
    "            print(f\"KeyphraseCountVectorizer failed, falling back to default: {e}\")\n",
    "            return kw_bert_model.extract_keywords(\n",
    "                docs=doc,\n",
    "                use_mmr=True,\n",
    "                diversity=diversity,\n",
    "                top_n=top_n\n",
    "            )\n",
    "\n",
    "    if use_keyllm:\n",
    "        # Get initial keywords from KeyBERT (limited to 20 for LLM processing)\n",
    "        initial_keywords = extract_with_keybert(20)\n",
    "        initial_keyword_texts = [kw[0] for kw in initial_keywords]\n",
    "        \n",
    "        # Refine using KeyLLM\n",
    "        refined_keywords = kw_llm_model.extract_keywords(\n",
    "            docs=doc,\n",
    "            candidate_keywords=initial_keywords\n",
    "        )[0]\n",
    "        \n",
    "        # Combine and deduplicate candidates\n",
    "        all_candidates = list(set(initial_keyword_texts) | set(refined_keywords))\n",
    "        all_candidates = [c for c in all_candidates if c]  # Remove empty strings\n",
    "        \n",
    "        # Get final keywords based on similarity\n",
    "        return get_top_kw(doc, all_candidates, model, top_n)\n",
    "    \n",
    "    # Use KeyBERT only\n",
    "    return extract_with_keybert(top_n)\n",
    "\n",
    "def create_vector_db(chunks, model, kw_bert_model, kw_llm_model, use_keyllm, diversity, top_n):\n",
    "    \"\"\"\n",
    "    Create a vector database from text chunks using keyword extraction.\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of text chunks to process\n",
    "        model: SentenceTransformer model for encoding\n",
    "        kw_bert_model: KeyBERT model instance\n",
    "        kw_llm_model: Optional KeyLLM model instance\n",
    "        use_keyllm: Whether to use KeyLLM\n",
    "        diversity: Diversity parameter for MMR\n",
    "        top_n: Number of keywords per chunk\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping keyword embeddings to associated chunks\n",
    "    \"\"\"\n",
    "    chunk_keywords_dict = {}\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        # Extract keywords for current chunk\n",
    "        keywords = extract_keywords_from_text(\n",
    "            doc=chunk,\n",
    "            kw_bert_model=kw_bert_model,\n",
    "            kw_llm_model=kw_llm_model,\n",
    "            model=model,\n",
    "            use_keyllm=use_keyllm,\n",
    "            diversity=diversity,\n",
    "            top_n=top_n\n",
    "        )\n",
    "        \n",
    "        # Create sorted keyword string and generate embedding\n",
    "        keywords_str = \" \".join(sorted(kw[0] for kw in keywords))\n",
    "        keywords_emb = tuple(model.encode(keywords_str).tolist())\n",
    "        \n",
    "        # Store chunk with its keyword embedding\n",
    "        chunk_keywords_dict.setdefault(keywords_emb, []).append(chunk)\n",
    "    \n",
    "    return chunk_keywords_dict\n",
    "\n",
    "def retrieve_documents(chunk_keywords_dict, question_emb, top_n):\n",
    "    \"\"\"\n",
    "    Retrieves the most relevant document chunks based on the similarity between \n",
    "    a given question embedding and the keyword embeddings of the chunks.\n",
    "\n",
    "    Args:\n",
    "        chunk_keywords_dict (dict): A dictionary where keys are keyword embeddings \n",
    "                                     (tuples), and values are lists of document chunks.\n",
    "        question_emb (array-like): The embedding of the question or query to compare against.\n",
    "        top_n (int, optional): The number of top relevant chunks to return. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of the top_n most relevant document chunks based on the cosine similarity \n",
    "              between the question embedding and the chunk keyword embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    keys = list(chunk_keywords_dict.keys())\n",
    "    similarity_scores = cosine_similarity([list(question_emb)], [list(key) for key in keys])[0]\n",
    "    top_indices = np.argsort(similarity_scores)[-top_n:][::-1]\n",
    "    top_keys = [keys[i] for i in top_indices]\n",
    "    retrieved_chunks = [chunk for key in top_keys for chunk in chunk_keywords_dict[key]]\n",
    "    return retrieved_chunks[:top_n]\n",
    "\n",
    "def get_answer_with_summary(question_answerer, summarizer, question, context, sum_max_length, sum_min_length):\n",
    "    \"\"\"\n",
    "    Retrieves an answer to the question from the context and generates a summary.\n",
    "\n",
    "    Args:\n",
    "        question_answerer (callable): Function to get an answer from the context.\n",
    "        summarizer (callable): Function to generate a summary of the context.\n",
    "        question (str): The question to answer.\n",
    "        context (str): The context from which the answer is derived.\n",
    "        sum_max_length (int): Max length of the summary.\n",
    "        sum_min_length (int): Min length of the summary.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The answer and the generated summary.\n",
    "    \"\"\"\n",
    "    \n",
    "    answer = question_answerer(question=question, context=context)\n",
    "    summary = summarizer(context[:4500], max_length=sum_max_length, min_length=sum_min_length, do_sample=False)[0]['summary_text']\n",
    "    summary = 'None'\n",
    "    return answer, summary\n",
    "\n",
    "class DocumentQuestionAnsweringPipeline:\n",
    "    \"\"\"\n",
    "    A pipeline for processing documents, extracting keywords, and answering questions \n",
    "    by leveraging embeddings, keyword extraction, and summarization.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_model='all-MiniLM-L6-v2', llm_model='Qwen/Qwen2.5-3B', qa_model='google-bert/bert-large-cased-whole-word-masking-finetuned-squad', sum_model='facebook/bart-large-cnn'):\n",
    "        \"\"\"\n",
    "        Initializes the pipeline with the required models for keyword extraction, \n",
    "        question answering, and summarization.\n",
    "        \n",
    "        Args:\n",
    "            embedding_model (str): Model used for generating document embeddings.\n",
    "            llm_model (str): Large Language Model used for keyword extraction.\n",
    "            qa_model (str): Model used for question answering.\n",
    "            sum_model (str): Model used for summarization.\n",
    "        \"\"\"\n",
    "        # Initialize models\n",
    "        self.model, self.kw_bert_model, self.kw_llm_model, self.question_answerer, self.summarizer = initialize_models(\n",
    "            embedding_model, llm_model, qa_model, sum_model)\n",
    "        self.chunk_keywords_dict = {}\n",
    "\n",
    "    def add_documents(self, docs, max_tokens=1000, overlap_percentage=0.5, use_keyllm=False, diversity=0.3, top_n_kw=10):\n",
    "        \"\"\"\n",
    "        Splits documents into chunks and extracts keyword embeddings for each chunk.\n",
    "        \n",
    "        Args:\n",
    "            docs (list): List of documents to process.\n",
    "            max_tokens (int): Maximum number of tokens per chunk.\n",
    "            overlap_percentage (float): Overlap between consecutive chunks.\n",
    "            use_keyllm (bool): Whether to use KeyLLM for keyword extraction.\n",
    "            diversity (float): Controls the diversity of keyword selection.\n",
    "            top_n_kw (int): Number of top keywords to extract from each chunk.\n",
    "        \"\"\"\n",
    "        all_chunks = []\n",
    "        # Split documents into chunks and extract keyword embeddings\n",
    "        for doc in docs:\n",
    "            chunks = split_document(doc, max_tokens, overlap_percentage)\n",
    "            all_chunks.extend(chunks)\n",
    "\n",
    "        # Create a vector database of chunk keywords and update the chunk_keywords_dict\n",
    "        new_chunk_keywords = create_vector_db(all_chunks, self.model, self.kw_bert_model, self.kw_llm_model, \n",
    "                                              use_keyllm, diversity, top_n_kw)\n",
    "        self.chunk_keywords_dict = new_chunk_keywords\n",
    "\n",
    "    def answer_question(self, question, use_keyllm=False, diversity=0.3, top_n_chunk=10, top_n_kw=10, \n",
    "                        sum_max_length=130, sum_min_length=30):\n",
    "        \"\"\"\n",
    "        Answers a question based on the context retrieved from relevant document chunks.\n",
    "        \n",
    "        Args:\n",
    "            question (str): The question to answer.\n",
    "            use_keyllm (bool): Whether to use KeyLLM for keyword extraction.\n",
    "            diversity (float): Controls the diversity of keyword selection.\n",
    "            top_n_chunk (int): Number of top chunks to retrieve.\n",
    "            top_n_kw (int): Number of top keywords to extract from the question.\n",
    "            sum_max_length (int): Maximum length of the summary.\n",
    "            sum_min_length (int): Minimum length of the summary.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: A tuple containing the answer to the question and the generated summary.\n",
    "        \"\"\"\n",
    "        # Extract keywords from the question\n",
    "        question_kw = extract_keywords_from_text(question, self.kw_bert_model, self.kw_llm_model, self.model, use_keyllm, \n",
    "                                                 diversity, top_n_kw)\n",
    "        question_kw_str = \", \".join(sorted([key[0] for key in question_kw]))\n",
    "        question_emb = tuple(self.model.encode(question_kw_str))\n",
    "        \n",
    "        # Retrieve relevant document chunks and build context\n",
    "        retrieved_chunks = retrieve_documents(self.chunk_keywords_dict, question_emb, top_n_chunk)\n",
    "        context = \" \".join(retrieved_chunks)\n",
    "        # Generate answer and summary\n",
    "        answer, summary = get_answer_with_summary(self.question_answerer, self.summarizer, question, context, \n",
    "                                                  sum_max_length, sum_min_length)\n",
    "        return answer, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T14:58:32.809845Z",
     "iopub.status.busy": "2025-02-15T14:58:32.809519Z",
     "iopub.status.idle": "2025-02-15T14:58:32.840620Z",
     "shell.execute_reply": "2025-02-15T14:58:32.839373Z",
     "shell.execute_reply.started": "2025-02-15T14:58:32.809817Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Example documents\n",
    "docs = \"France is a country in Europe, bordered by Spain, Germany, and Italy.\"\n",
    "docs = [docs]\n",
    "# Example question\n",
    "question = \"What is the capital of France?\"\n",
    "\n",
    "# Initialize the pipeline\n",
    "qa_pipeline = DocumentQuestionAnsweringPipeline()\n",
    "    \n",
    "# Add documents to the pipeline\n",
    "qa_pipeline.add_documents(docs=docs)\n",
    "\n",
    "# Answer a question\n",
    "answer, summary = qa_pipeline.answer_question(question=question)\n",
    "\n",
    "# Print the result\n",
    "print(\"Answer:\", answer)\n",
    "print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test QA Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T14:58:37.153670Z",
     "iopub.status.busy": "2025-02-15T14:58:37.153353Z",
     "iopub.status.idle": "2025-02-15T14:58:37.162833Z",
     "shell.execute_reply": "2025-02-15T14:58:37.162050Z",
     "shell.execute_reply.started": "2025-02-15T14:58:37.153641Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def normalize_text(s):\n",
    "    \"\"\"\n",
    "    Normalizes a string by:\n",
    "    - Converting to lowercase\n",
    "    - Removing punctuation\n",
    "    - Removing articles ('a', 'an', 'the')\n",
    "    - Removing extra spaces\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return \"\"\n",
    "\n",
    "    # Convert to lowercase\n",
    "    s = s.lower()\n",
    "    # Remove punctuation\n",
    "    s = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", s)\n",
    "    # Remove articles\n",
    "    s = re.sub(r\"\\b(a|an|the)\\b\", \" \", s)\n",
    "    # Remove extra spaces\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def get_tokens_from_text(s):\n",
    "    \"\"\"Returns the normalized tokens from a string.\"\"\"\n",
    "    return normalize_text(s).split() if s else []\n",
    "\n",
    "def calculate_exact_match(predicted, true_answers):\n",
    "    \"\"\"\n",
    "    Calculates the Exact Match between the predicted answer and true answers.\n",
    "    Returns 1.0 if there is a match, otherwise 0.0.\n",
    "    \"\"\"\n",
    "    true_texts = true_answers['text'] if isinstance(true_answers, dict) else true_answers\n",
    "    for t in true_texts:\n",
    "        if normalize_text(predicted) == normalize_text(t):\n",
    "            return 1.0\n",
    "    return 0.0\n",
    "\n",
    "def calculate_f1_score(predicted, true_answers):\n",
    "    true_texts = true_answers['text'] if isinstance(true_answers, dict) else true_answers\n",
    "    f1_scores = []\n",
    "    for true_answer in true_texts:\n",
    "        pred_tokens = get_tokens_from_text(predicted)\n",
    "        true_tokens = get_tokens_from_text(true_answer)\n",
    "        common = Counter(pred_tokens) & Counter(true_tokens)\n",
    "        num_same = sum(common.values())\n",
    "        if num_same == 0:\n",
    "            f1_scores.append(0.0)\n",
    "            continue\n",
    "        precision = num_same / len(pred_tokens)\n",
    "        recall = num_same / len(true_tokens)\n",
    "        f1_scores.append((2 * precision * recall) / (precision + recall) if (precision + recall) else 0.0)\n",
    "    return max(f1_scores) if f1_scores else 0.0\n",
    "\n",
    "def calculate_metrics(results):\n",
    "    \"\"\"\n",
    "    Calculates the main metrics: Exact Match (EM), F1 score, and average confidence.\n",
    "    \"\"\"\n",
    "    exact_matches = []\n",
    "    f1_scores = []\n",
    "    confidences = []\n",
    "\n",
    "    for result in results:\n",
    "        pred_answer = result[\"predicted_answer\"]\n",
    "        true_answers = result[\"true_answers\"]\n",
    "        exact_matches.append(calculate_exact_match(pred_answer, true_answers))\n",
    "        f1_scores.append(calculate_f1_score(pred_answer, true_answers))\n",
    "        confidences.append(result[\"confidence\"])\n",
    "    \n",
    "    total = len(results)\n",
    "    eval_dict = OrderedDict([\n",
    "        ('exact', 100.0 * np.mean(exact_matches)),\n",
    "        ('f1', 100.0 * np.mean(f1_scores)),\n",
    "        ('avg_confidence', np.mean(confidences)),\n",
    "        ('total', total)\n",
    "    ])\n",
    "    \n",
    "    return eval_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T14:58:39.552024Z",
     "iopub.status.busy": "2025-02-15T14:58:39.551611Z",
     "iopub.status.idle": "2025-02-15T14:58:39.563601Z",
     "shell.execute_reply": "2025-02-15T14:58:39.562653Z",
     "shell.execute_reply.started": "2025-02-15T14:58:39.551974Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_large_corpus(qa_pipeline, param_combinations, dataset):\n",
    "    \"\"\"\n",
    "    Performs the evaluation of the pipeline on a grid search of parameters for a large corpus\n",
    "    (with or without noise). Returns the best configurations and metrics for each model.\n",
    "\n",
    "    Parameters:\n",
    "      - dataset: DataFrame with 'context', 'question', 'answers'.\n",
    "\n",
    "    Returns:\n",
    "      - best_configs: Best configurations based on F1 score.\n",
    "      - results_with_metrics: Metrics and configurations for all parameter combinations.\n",
    "    \"\"\"\n",
    "    unique_contexts = dataset['context'].drop_duplicates().tolist()\n",
    "    large_corpus = \" \".join(unique_contexts)\n",
    "\n",
    "    results_with_metrics = []  # Store results and metrics\n",
    "    start_time = time.time()  # Start timing the entire grid search\n",
    "\n",
    "    print(\"\\nStarting grid search...\")\n",
    "\n",
    "    for params in tqdm(param_combinations, position=0, desc=\"Testing configurations\"):\n",
    "        param_start_time = time.time()  # Start timing for this combination\n",
    "\n",
    "        # Add documents to the pipeline with the current parameters\n",
    "        qa_pipeline.add_documents(\n",
    "            [large_corpus],\n",
    "            max_tokens=params['max_tokens'],\n",
    "            overlap_percentage=params['overlap_percentage'],\n",
    "            use_keyllm=params['use_keyllm'],\n",
    "            diversity=params['doc_diversity'],\n",
    "            top_n_kw=params['doc_top_n_kw']\n",
    "        )\n",
    "        print(\"Large Corpus Loaded\")\n",
    "        \n",
    "        results = []\n",
    "\n",
    "        for _, row in dataset.iterrows():\n",
    "            # Run QA pipeline to get predictions\n",
    "            answer, _ = qa_pipeline.answer_question(\n",
    "                question=row['question'],\n",
    "                use_keyllm=params['use_keyllm'],\n",
    "                diversity=params['question_diversity'],\n",
    "                top_n_chunk=params['top_n_chunks'],\n",
    "                top_n_kw=params['question_top_n_kw']\n",
    "            )\n",
    "            results.append({\n",
    "                \"predicted_answer\": answer['answer'],\n",
    "                \"true_answers\": row['answers'],\n",
    "                \"confidence\": answer['score']\n",
    "            })\n",
    "\n",
    "        # Calculate metrics for this parameter combination\n",
    "        metrics = calculate_metrics(results)\n",
    "        model_key = f\"{params['qa_model']}_{params['embedding_model']}\"\n",
    "        results_with_metrics.append({\n",
    "            \"params\": params,\n",
    "            \"metrics\": metrics,\n",
    "            \"model_key\": model_key\n",
    "        })\n",
    "        \n",
    "        pprint({\n",
    "            \"params\": params,\n",
    "            \"metrics\": metrics,\n",
    "            \"model_key\": model_key\n",
    "        })\n",
    "        # Log time for this parameter combination\n",
    "        param_time = timedelta(seconds=(time.time() - param_start_time))\n",
    "        print(f\"Completed configuration {params} in {param_time}.\")\n",
    "\n",
    "    # End timing the grid search\n",
    "    total_time = timedelta(seconds=(time.time() - start_time))\n",
    "    print(f\"\\nGrid search completed in {total_time}.\")\n",
    "\n",
    "    # Select the best configuration for each model based on F1 score\n",
    "    best_configs = {}\n",
    "    for res in results_with_metrics:\n",
    "        key = res[\"model_key\"]\n",
    "        if key not in best_configs or res[\"metrics\"][\"f1\"] > best_configs[key][\"metrics\"][\"f1\"]:\n",
    "            best_configs[key] = res\n",
    "\n",
    "    return best_configs, results_with_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T15:14:01.635122Z",
     "iopub.status.busy": "2025-02-15T15:14:01.634778Z",
     "iopub.status.idle": "2025-02-15T15:14:18.263610Z",
     "shell.execute_reply": "2025-02-15T15:14:18.262688Z",
     "shell.execute_reply.started": "2025-02-15T15:14:01.635092Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "qa_pipeline = DocumentQuestionAnsweringPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-02-15T15:16:02.184363Z",
     "iopub.status.busy": "2025-02-15T15:16:02.184037Z",
     "iopub.status.idle": "2025-02-15T16:38:41.419929Z",
     "shell.execute_reply": "2025-02-15T16:38:41.419260Z",
     "shell.execute_reply.started": "2025-02-15T15:16:02.184337Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the SQuAD dataset\n",
    "squad_dataset = load_dataset(\"rajpurkar/squad\")\n",
    "validation_set = squad_dataset['validation']\n",
    "validation_df = validation_set.to_pandas()[['context', 'question', 'answers']].head(500)\n",
    "\n",
    "\n",
    "# Define parameters for Grid Search\n",
    "param_grid = {\n",
    "    \"qa_model\": [\"google-bert/bert-large-cased-whole-word-masking-finetuned-squad\"], #default qa model\n",
    "    \"embedding_model\": [\"all-MiniLM-L6-v2\"], #default embedding model\n",
    "    \"llm_model\":[\"Qwen/Qwen2.5-3B\"], #default llm model\n",
    "    \"use_keyllm\": [True, False],\n",
    "    \"doc_diversity\": [0.3],\n",
    "    \"doc_top_n_kw\": [10],\n",
    "    \"max_tokens\": [200, 300, 500, 1000], \n",
    "    \"overlap_percentage\": [0.1, 0.3, 0.5],\n",
    "    \"question_diversity\": [0.3],\n",
    "    \"question_top_n_kw\": [10],\n",
    "    \"top_n_chunks\": [1,3,5,10]\n",
    "}\n",
    "\n",
    "# Generate all possible parameter combinations\n",
    "keys = list(param_grid.keys())\n",
    "param_combinations = [dict(zip(keys, values)) for values in product(*[param_grid[k] for k in keys])]\n",
    "\n",
    "# run the grid search\n",
    "best_configs, results_with_metrics = evaluate_large_corpus(qa_pipeline, param_combinations, validation_df)\n",
    "\n",
    "# print results\n",
    "print(f\"\\nBest configurations:\")\n",
    "for model_key, config in best_configs.items():\n",
    "    print(f\"\\nModel: {model_key}\")\n",
    "    print(\"Document parameters:\")\n",
    "    print(f\"  Diversity: {config['params']['doc_diversity']}\")\n",
    "    print(f\"  Max tokens: {config['params']['max_tokens']}\")\n",
    "    print(f\"  Top keywords: {config['params']['doc_top_n_kw']}\")\n",
    "    print(f\"  Overlap percentage: {config['params']['overlap_percentage']}\")\n",
    "    print(\"Question parameters:\")\n",
    "    print(f\"  Diversity: {config['params']['question_diversity']}\")\n",
    "    print(f\"  Top chunks: {config['params']['top_n_chunks']}\")\n",
    "    print(f\"  Top keywords: {config['params']['question_top_n_kw']}\")\n",
    "    print(f\"\\nMetrics:\")\n",
    "    print(f\"  Exact match: {config['metrics']['exact']:.2f}\")\n",
    "    print(f\"  F1 score: {config['metrics']['f1']:.2f}\")\n",
    "    print(f\"  Confidence: {config['metrics']['avg_confidence']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6646477,
     "sourceId": 10721994,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
